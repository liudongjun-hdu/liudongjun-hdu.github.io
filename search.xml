<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Linux服务器环境配置</title>
      <link href="2021/03/22/Linux%E6%9C%8D%E5%8A%A1%E5%99%A8%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/"/>
      <url>2021/03/22/Linux%E6%9C%8D%E5%8A%A1%E5%99%A8%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/</url>
      
        <content type="html"><![CDATA[<h1 id="Linux服务器环境部署"><a href="#Linux服务器环境部署" class="headerlink" title="Linux服务器环境部署"></a>Linux服务器环境部署</h1><p>by 栋栋</p><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><ol><li>vim操作：<ol><li>vim xxxxx</li><li>按i 进入编辑模式，光标移到对应位置进行插入</li><li>编辑好文本按Esc退出插入状态</li><li>保存退出命令 ：wq </li><li>删除一个目录，如（Anaconda3-5.3.1）：sudo rm -rf Anaconda3-5.3.1<h2 id="创建用户"><a href="#创建用户" class="headerlink" title="创建用户"></a>创建用户</h2></li></ol></li><li>获取权限：sudo -i，切换至 root 超级用户目录 </li><li>创建用户：adduser ldj(你的名字) </li><li>设置密码：passwd ldj xxx(你的密码) </li><li>切换用户：su ldj 进入自己的用户目录 </li><li>如果你需要管理员权限的话，可切换至root目录，赋予用户文件修改权限： <ol><li>chmod u+w /etc/sudoers </li><li>vim /etc/sudoers 进入文件，添加语句：ldj ALL=(ALL) ALL </li><li>“:wq”保存并退出文件 </li><li>usermod -g root ldj 修改属组关系 </li><li>vim /etc/passwd 记住自己的ID，不要修改因为只是获取root权限而不是变成root用户</li><li>“:wq”保存并退出文件 </li><li>关闭文件修改权限：chmod u-w /etc/sudoers <h2 id="安装-anaconda"><a href="#安装-anaconda" class="headerlink" title="安装 anaconda"></a>安装 anaconda</h2></li><li>下载 anaconda：sudo wget <a href="https://mirrors.tuna.tsinghua.edu.cn/anaconda/archive/Anaconda3-5.3">https://mirrors.tuna.tsinghua.edu.cn/anaconda/archive/Anaconda3-5.3</a>. 1-Linux-x86_64.sh </li><li>安装 anaconda：sudo bash Anaconda3-5.3.1-Linux-x86_64.sh(提前将文件下载到你的目录)</li><li>配置环境变量：</li><li>编辑配置文件：vi ~/.bashrc（配置该用户的环境变量）</li><li>最后添加一行：export PATH=/home/ldj/anaconda3/bin:$PATH</li><li>激活安装：source ~/.bashrc</li><li>验证是否安装成功：conda –version 查看版本，conda list 查看包列表<h2 id="安装-cuda-和-cudnn"><a href="#安装-cuda-和-cudnn" class="headerlink" title="安装 cuda 和 cudnn"></a>安装 cuda 和 cudnn</h2></li></ol></li><li>版本匹配表<br><img src="/images/versionmatching1.jpg" alt="版本匹配表"><br><img src="/images/versionmatching2.jpg" alt="版本匹配表"><br>注释：因为我们实验室服务器已经安装好驱动了，为了减少篇幅，所以省略安装驱动步骤，有需要可以自行百度。<br>nvidia-smi，有如下显示则说明驱动安装成功：<br><img src="/images/versionmatching3.jpg" alt="版本匹配表"></li><li>linux 服务器安装 cuda <ol><li>下载 cuda:sudo wget -c <a href="https://developer.nvidia.com/compute/cuda/9.0/Prod/local_installers/cuda_9.0.176_384.81_linux-run">https://developer.nvidia.com/compute/cuda/9.0/Prod/local_installers/cuda_9.0.176_384.81_linux-run</a></li><li>安装 cuda:sudo sh cuda_9.0.176_384.81_linux-run </li></ol></li><li>linux 服务器安装 cudnn <ol><li>下载 cudnn：sudo wget -c <a href="https://developer.nvidia.com/compute/machine-learning/cudnn/secure/v7.1.4/prod/9.0_20180516/cudnn-9.0-linux-x64-v7.1">https://developer.nvidia.com/compute/machine-learning/cudnn/secure/v7.1.4/prod/9.0_20180516/cudnn-9.0-linux-x64-v7.1</a> </li><li>安装 cudnn：tar -xzvf /home/ldj/cuda/cudnn-9.0-linux-x64-v7.1.tgz -C </li><li>配置 cudnn: <ol><li>将cudnn的文件移动到cuda文件目录下</li><li>cuda软连接地址在安装过程中会建议一个软连接：会在/usr/local/下面建立一个软连接cuda该软连接连接到安装的真正的cuda-10.0的地址。</li><li>软连接的建立可以用于多个版本的cuda的管理。<br>sudo cp cuda/include/cudnn.h    /usr/local/cuda-9.0/include<br>sudo cp cuda/lib64/libcudnn*    /usr/local/cuda-9.0/lib64<br>sudo chmod a+r /usr/local/cuda-9.0/include/cudnn.h   /usr/local/cuda-9.0/lib64/libcudnn*</li></ol></li></ol></li><li>配置用户环境变量:<br>sudo vim ~/.bashrc<br>export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/cuda/lib64<br>export PATH=$PATH:/usr/local/cuda/bin</li><li>激活：source ~/.bashrc 使新配置的环境变量生效 </li><li>验证是否成功：nvcc -V：查看是否安装成功 </li><li>查看位置：which NV：查看 NV 位置 <h2 id="配置个人环境"><a href="#配置个人环境" class="headerlink" title="配置个人环境"></a>配置个人环境</h2></li><li>在 anaconda 下建立自己的虚拟环境： <ol><li>建立环境：conda create -n name python=3.6,name:虚拟环境 名称，python 版本自己更改 </li><li>查看环境：conda env list （查看所有环境） </li><li>激活环境：source activate name（环境名称为 name），进入创建的 name 环境 </li></ol></li><li>安装 tensorflow-gpu 版，输入 pip install tensorflow-gpu==1.8.0,版本号自己更换。<br>输入 python 回车，输入 import tensorflow，如果没有报错说明 tensorflow 配置成功。<br>也可使用下面的代码进行测试： <pre><code>import tensorflow as tf hello = tf.constant(&#39;Hello, TensorFlow!&#39;)sess = tf.Session() print(sess.run(hello)) </code></pre></li><li>安装 keras-gpu 版，输入 pip install keras-gpu==2.1.4,版本号 自己更换。<br>输入 python 回车，输入 import keras，如果没有报错说明 keras 配置成功，其他环境同上方法配置 </li><li>关闭环境：source deactivate py36_wyh<h2 id="安装-pycharm"><a href="#安装-pycharm" class="headerlink" title="安装 pycharm"></a>安装 pycharm</h2></li><li>下载 pycharm：<a href="https://www.jetbrains.com/pycharm/download/">https://www.jetbrains.com/pycharm/download/</a> </li><li>安装 pycharm： tar -zxvf pycharm-community-2020.2.3.tar.gz 3.启动 pycharm：bin 文件夹下输入./pycharm.sh 启动 pycharm<br>cd /home/ldj/pycharm-community-2020.2.3/bin/<br>./pycharm.sh <h2 id="后记"><a href="#后记" class="headerlink" title="后记"></a>后记</h2></li><li>安装一定要在自己的目录下进行，以免更改他人环境 </li><li>在安装之前配置清华镜像，下载速度会很快，更换下载源的方法如 下: 在自己的目录下新建一个.pip 文件夹，在.pip 文件夹下新建 pip.conf 文件，内容为：<br>[global] index-url = <a href="https://pypi.douban.com/simple/">https://pypi.douban.com/simple/</a><br>[install] trusted-host = <a href="https://pypi.douban.com/simple/">https://pypi.douban.com/simple/</a><br>几个常用的国内源，大家可以自己选择，替换文件中的网址就可以：<br>阿里云 <a href="http://mirrors.aliyun.com/pypi/simple/">http://mirrors.aliyun.com/pypi/simple/</a> 中国科技大学 <a href="https://pypi.mirrors.ustc.edu.cn/simple/">https://pypi.mirrors.ustc.edu.cn/simple/</a><br>豆瓣(douban) <a href="http://pypi.douban.com/simple/">http://pypi.douban.com/simple/</a> 清华大学 <a href="https://pypi.tuna.tsinghua.edu.cn/simple/">https://pypi.tuna.tsinghua.edu.cn/simple/</a><br>中国科学技术大学 <a href="http://pypi.mirrors.ustc.edu.cn/simple/">http://pypi.mirrors.ustc.edu.cn/simple/</a> </li><li>conda更换清华源<br>conda config –add channels <a href="https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/">https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/</a><br>conda config –set show_channel_urls yes<br>编辑conda的配置文件：vi ~/.condarc<br>删除第三行“- defaults”，然后wq保存退出即可。</li><li>实验室的cuda和cudnn一般都是配置好的，直接引用即可。避免重复下载，浪费空间。</li><li>跑程序时，加上 os.environ[“CUDA_VISIBLE_DEVICES”]=”0”,0 是 GPU 标志，更换数字即更换 GPU </li><li>注意区分命令前有无 sudo 的区别，sudo为增加权限使用。 </li><li>升级pip：pip install –upgrade pip</li><li>启动pyCharm的命令设置别名：<br><a href="https://blog.csdn.net/hay54/article/details/82344014">https://blog.csdn.net/hay54/article/details/82344014</a><br>alias luminarypy=’cd /home/wyh/pycharm-community-2020.2.3/bin/;./pycharm.sh’<br>luminarypy可打开pycharm<br>alias命令，查看所有别名<br>unalias 别名，表示取消某个别名。<br>注意：alias命令只作用于当次登入的操作。如果想每次登入都能使用这些命令的别名，则可以把相应的alias命令存放在 ~/.bashrc 文件中<br>操作：vi ~/.bashrc<br>找到alias命令那边加入：alias luminarypy=’cd /home/wyh/pycharm-community-2020.2.3/bin/;./pycharm.sh’<br>然后激活：source ~/.bashrc<br>这样别名就永久生效了<h2 id="作者"><a href="#作者" class="headerlink" title="作者"></a>作者</h2>杭州电子科技大学计算机学院 lab519 刘栋军 联系方式：<a href="mailto:&#x6c;&#x69;&#117;&#100;&#x6f;&#110;&#103;&#x6a;&#117;&#110;&#x40;&#104;&#100;&#x75;&#46;&#x65;&#100;&#x75;&#x2e;&#x63;&#x6e;">&#x6c;&#x69;&#117;&#100;&#x6f;&#110;&#103;&#x6a;&#117;&#110;&#x40;&#104;&#100;&#x75;&#46;&#x65;&#100;&#x75;&#x2e;&#x63;&#x6e;</a> <h2 id="备注"><a href="#备注" class="headerlink" title="备注"></a>备注</h2>本文部分引用 csdn 博客等网络内容，如有侵权，请联系本人删除， 谢谢。<br>本人才疏学浅，如有错误，请联系本人更改，谢谢。</li></ol>]]></content>
      
      
      
        <tags>
            
            <tag> Linux </tag>
            
            <tag> 服务器 </tag>
            
            <tag> 环境部署 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>字体设置</title>
      <link href="2021/03/22/%E5%AD%97%E4%BD%93%E8%AE%BE%E7%BD%AE/"/>
      <url>2021/03/22/%E5%AD%97%E4%BD%93%E8%AE%BE%E7%BD%AE/</url>
      
        <content type="html"><![CDATA[<ol><li>设置,路径是你的字体文件存放的路径<pre><code>zhfont1 = matplotlib.font_manager.FontProperties(fname=&#39;/usr/share/fonts/my_fonts/simhei.ttf&#39;)</code></pre></li><li>使用<pre><code>plt.xlabel(&#39;预测类别&#39;, fontsize=20, fontproperties=zhfont1)plt.ylabel(&#39;真实类别&#39;, fontsize=20, fontproperties=zhfont1)</code></pre></li></ol>]]></content>
      
      
      
        <tags>
            
            <tag> 字体 </tag>
            
            <tag> 中英文 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>绘图</title>
      <link href="2021/03/21/%E7%BB%98%E5%9B%BE/"/>
      <url>2021/03/21/%E7%BB%98%E5%9B%BE/</url>
      
        <content type="html"><![CDATA[<ol><li><p>绘图需要引入的包以及图片存储路径<br>import matplotlib.pyplot as plt<br>import matplotlib<br>from sklearn.metrics import *<br>import matplotlib.pyplot as plt<br>import numpy as np<br>exp_root = ‘/home/wyh/Python_workspace/DAN_EEGIMAGE_CODE/model_indices/‘</p></li><li><p>横向对比图</p><pre><code>def Comparsion_regression(): # 构建数据 x_data = [&#39;愤怒&#39;, &#39;快乐&#39;, &#39;中立&#39;, &#39;悲伤&#39;, &#39;惊奇&#39;, &#39;厌恶&#39;, &#39;恐惧&#39;] y_data1 = [100.00, 92.31, 100.00, 83.33, 83.33, 50.00, 37.50] y_data2 = [100.00, 96.15, 100.00, 83.33, 91.67, 62.50, 37.50] bar_width = 0.4 # Y轴数据使用range(len(x_data), 就是0、1、2... plt.barh(y=range(len(x_data)), width=y_data1, label=&#39;IMAGE&#39;, color=&#39;steelblue&#39;, alpha=0.8, height=bar_width) # Y轴数据使用np.arange(len(x_data))+bar_width, # 就是bar_width、1+bar_width、2+bar_width...这样就和第一个柱状图并列了 plt.barh(y=np.arange(len(x_data)) + bar_width, width=y_data2, label=&#39;EEG-LIKE&#39;, color=&#39;indianred&#39;, alpha=0.8,          height=bar_width) # 在柱状图上显示具体数值, ha参数控制水平对齐方式, va控制垂直对齐方式 for y, x in enumerate(y_data1):     plt.text(x - 6, y - bar_width / 2, &#39;%s&#39; % (x), ha=&#39;center&#39;, va=&#39;bottom&#39;) for y, x in enumerate(y_data2):     plt.text(x - 6, y + bar_width / 2, &#39;%s&#39; % (x), ha=&#39;center&#39;, va=&#39;bottom&#39;) # 为Y轴设置刻度值 plt.yticks(np.arange(len(x_data)) + bar_width / 2, x_data) # 设置标题 plt.title(&quot;Comparison graph of accuracy before and after regression&quot;) # 为两条坐标轴设置名称 plt.xlabel(&quot;Accuracy(%)&quot;) plt.ylabel(&quot;Emotional categories&quot;) plt.subplots_adjust(left=0.2, right=0.8, top=0.8, bottom=0.2) # 显示图例 plt.legend(loc=&#39;upper right&#39;) plt.savefig(&quot;/home/wyh/Python_workspace/DAN_EEGIMAGE_CODE/model_indices/DualChart.jpg&quot;) plt.show()</code></pre><p><img src="/images/DualChart.jpg" alt="横向对比图"></p></li><li><p>混淆矩阵</p><pre><code>def plot_confusion_matrix(y_true, y_pred, labels):  # 计算混淆矩阵，以评估分类的准确性。 cmap = plt.cm.binary cm = confusion_matrix(y_true, y_pred) tick_marks = np.array(range(len(labels))) + 0.5 np.set_printoptions(precision=2) cm_normalized = cm.astype(&#39;float&#39;) / cm.sum(axis=1)[:, np.newaxis] plt.figure(figsize=(20, 18), dpi=120) ind_array = np.arange(len(labels)) x, y = np.meshgrid(ind_array, ind_array) intFlag = 0 for x_val, y_val in zip(x.flatten(), y.flatten()):     #     if (intFlag):         c = cm[y_val][x_val]         plt.text(x_val, y_val, &quot;%d&quot; % (c,), color=&#39;black&#39;, fontsize=20, va=&#39;center&#39;, ha=&#39;center&#39;)     else:         c = cm_normalized[y_val][x_val]         if (c &gt; 0.80):             plt.text(x_val, y_val, &quot;%0.2f&quot; % (c * 100,), color=&#39;white&#39;, fontsize=20, va=&#39;center&#39;, ha=&#39;center&#39;)         else:             plt.text(x_val, y_val, &quot;%0.2f&quot; % (c * 100,), color=&#39;black&#39;, fontsize=20, va=&#39;center&#39;, ha=&#39;center&#39;) if (intFlag):     plt.imshow(cm, interpolation=&#39;nearest&#39;, cmap=cmap) else:     plt.imshow(cm_normalized, interpolation=&#39;nearest&#39;, cmap=cmap) plt.gca().set_xticks(tick_marks, minor=True) plt.gca().set_yticks(tick_marks, minor=True) plt.gca().xaxis.set_ticks_position(&#39;none&#39;) plt.gca().yaxis.set_ticks_position(&#39;none&#39;) plt.grid(True, which=&#39;minor&#39;, linestyle=&#39;-&#39;) plt.gcf().subplots_adjust(bottom=0.15) plt.title(&#39;&#39;) plt.colorbar() xlocations = np.array(range(len(labels))) plt.xticks(xlocations, labels, rotation=90, fontsize=20, fontproperties=zhfont1) plt.yticks(xlocations, labels, fontsize=20, fontproperties=zhfont1) plt.ylabel(&#39;真实类别&#39;, fontsize=20, fontproperties=zhfont1) plt.xlabel(&#39;预测类别&#39;, fontsize=20, fontproperties=zhfont1) plt.savefig(exp_root + &#39;knn_confusion_matrix12.jpg&#39;, dpi=600) plt.show()</code></pre><p><img src="/images/knn_confusion_matrix12.jpg" alt="混淆矩阵图"></p></li><li><p>t-sne</p><pre><code>def plot_embedding(data, label): fig = plt.figure(figsize=(5, 5)) type1_x = [] type1_y = [] type2_x = [] type2_y = [] type3_x = [] type3_y = [] type4_x = [] type4_y = [] type5_x = [] type5_y = [] type6_x = [] type6_y = [] type7_x = [] type7_y = [] type8_x = [] type8_y = [] for i in range(data.shape[0]):     if label[i] == 0:         type1_x.append(data[i][0])         type1_y.append(data[i][1])     if label[i] == 1:         type2_x.append(data[i][0])         type2_y.append(data[i][1])     if label[i] == 2:         type3_x.append(data[i][0])         type3_y.append(data[i][1])     if label[i] == 3:         type4_x.append(data[i][0])         type4_y.append(data[i][1])     if label[i] == 4:         type5_x.append(data[i][0])         type5_y.append(data[i][1])     if label[i] == 5:         type6_x.append(data[i][0])         type6_y.append(data[i][1])     if label[i] == 6:         type7_x.append(data[i][0])         type7_y.append(data[i][1])     if label[i] == 7:         type8_x.append(data[i][0])         type8_y.append(data[i][1]) color = plt.cm.Set3(0) color = np.array(color).reshape(1, 4) color1 = plt.cm.Set3(1) color1 = np.array(color1).reshape(1, 4) color2 = plt.cm.Set3(2) color2 = np.array(color2).reshape(1, 4) color3 = plt.cm.Set3(3) color3 = np.array(color3).reshape(1, 4) type1 = plt.scatter(type1_x, type1_y, s=10, c=&#39;r&#39;) type2 = plt.scatter(type2_x, type2_y, s=10, c=&#39;g&#39;) type3 = plt.scatter(type3_x, type3_y, s=10, c=&#39;b&#39;) type4 = plt.scatter(type4_x, type4_y, s=10, c=&#39;k&#39;) type5 = plt.scatter(type5_x, type5_y, s=10, c=&#39;c&#39;) type6 = plt.scatter(type6_x, type6_y, s=10, c=&#39;m&#39;) type7 = plt.scatter(type7_x, type7_y, s=10, c=&#39;y&#39;) type8 = plt.scatter(type8_x, type8_y, s=10, c=color) plt.legend((type1, type2, type3, type4, type5, type6, type7, type8),            (&#39;愤怒&#39;, &#39;厌恶&#39;, &#39;恐惧&#39;, &#39;快乐&#39;, &#39;中立&#39;, &#39;悲伤&#39;, &#39;惊奇&#39;), prop=zhfont1) plt.xticks() plt.yticks() return figdef visual_features(x_train_img, x_test_img, y_train, y_test): from sklearn.manifold import TSNE # 可视化训练/测试数据模式 X_train = x_train_img Y_train = y_train tsne = TSNE(n_components=2, init=&#39;pca&#39;, random_state=0)  # 使用TSNE对特征降到二维 reduce_dim_X = tsne.fit_transform(X_train)  # 降维后的数据 # 画图 # plt.subplot(1, 2, 1) fig = plot_embedding(reduce_dim_X, Y_train) # 图例过大，保存figure时无法保存完全，故对此参数进行调整 fig.subplots_adjust(right=0.7) plt.title(&#39;图像视觉特征可视化&#39;, fontproperties=zhfont1) plt.savefig(&quot;/home/wyh/Python_workspace/DAN_EEGIMAGE_CODE/model_indices/t-sne_train_image.jpg&quot;) plt.show() X_test = x_test_img Y_test = y_test tsne = TSNE(n_components=2, init=&#39;pca&#39;, random_state=0)  # 使用TSNE对特征降到二维 reduce_dim_X = tsne.fit_transform(X_test)  # 降维后的数据 # 画图 # plt.subplot(1, 2, 2) fig = plot_embedding(reduce_dim_X, Y_test) # 图例过大，保存figure时无法保存完全，故对此参数进行调整 fig.subplots_adjust(right=0.7) plt.title(&#39;虚拟脑电情感特征可视化&#39;, fontproperties=zhfont1) # plt.show() plt.savefig(&quot;/home/wyh/Python_workspace/DAN_EEGIMAGE_CODE/model_indices/t-sne_train_EEG-LIKE.jpg&quot;) plt.show()</code></pre><p><img src="/images/t-sne_train_image.jpg" alt="t-sne图1"><br><img src="/images/t-sne_train_EEG-LIKE.jpg" alt="t-sne图2"></p></li><li><p>纵向对比图</p><pre><code>def comparison_bar(IMAGE_score, Regression_score, Fusion_score, EEG_score): X = [&#39;图像&#39;, &#39;回归&#39;, &#39;融合&#39;, &#39;脑电&#39;] Y = [0.839080459770114, 0.873563218390804, 0.885057471264367, 0.977011494252873] plt.xticks(fontproperties=zhfont1) plt.bar(X, Y, 0.5, color=[&#39;green&#39;, &#39;blue&#39;, &#39;yellow&#39;, &#39;red&#39;]) plt.xlabel(&quot;方法&quot;, fontproperties=zhfont1) plt.ylabel(&quot;准确率&quot;, fontproperties=zhfont1) plt.title(&quot;方法对比图&quot;, fontproperties=zhfont1) for a, b in zip(X, Y):     plt.text(a, b, &#39;%.4f&#39; % b, ha=&#39;center&#39;, va=&#39;bottom&#39;, fontsize=11) # plt.show() plt.savefig(&quot;/home/wyh/Python_workspace/DAN_EEGIMAGE_CODE/model_indices/BarChart.jpg&quot;) plt.show()</code></pre><p><img src="/images/BarChart.jpg" alt="纵向对比图"></p></li><li><p>折线图</p><pre><code>def EEG_like(x_test_eeg, y_pred_test): x_test_eeg_average = x_test_eeg.mean(0) x_test_like_average = y_pred_test.mean(0) x1 = range(0, 128) x2 = range(0, 128) y1 = x_test_eeg_average y2 = x_test_like_average plt.subplot(2, 1, 1) plt.plot(x1, y1, &#39;.-&#39;) plt.title(&#39;脑电情感特征和虚拟脑电情感特征&#39;, fontproperties=zhfont1) plt.ylabel(&#39;脑电情感特征&#39;, fontproperties=zhfont1) # plt.xlabel(&#39;Time series&#39;,fontproperties=zhfont1) plt.subplot(2, 1, 2) plt.plot(x2, y2, &#39;.-&#39;) plt.xlabel(&#39;模拟时间序列&#39;, fontproperties=zhfont1) plt.ylabel(&#39;虚拟脑电情感特征&#39;, fontproperties=zhfont1) # plt.show() plt.savefig(&quot;/home/wyh/Python_workspace/DAN_EEGIMAGE_CODE/model_indices/EEG-LIKE.jpg&quot;) plt.show()</code></pre><p><img src="/images/EEG-LIKE.jpg" alt="折线图"></p></li><li><p>accuracy_loss图</p><pre><code>x1 = range(0, iteration//log_interval)x2 = range(0, iteration//log_interval)y1 = Accuracy_listy2 = Loss_listplt.subplot(2, 1, 1)plt.plot(x1, y1, &#39;o-&#39;)plt.title(&#39;Train accuracy vs. epochs&#39;)plt.ylabel(&#39;Train accuracy&#39;)plt.subplot(2, 1, 2)plt.plot(x2, y2, &#39;.-&#39;)plt.xlabel(&#39;Train loss vs. epochs&#39;)plt.ylabel(&#39;Train loss&#39;)plt.savefig(&quot;/home/wyh/Python_workspace/DAN_EEGIMAGE_CODE/model_indices/accuracy_loss.jpg&quot;)plt.show()</code></pre><p><img src="/images/accuracy_loss.jpg" alt="accuracy_loss图"></p></li></ol>]]></content>
      
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> 绘图 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>深度学习零散知识点</title>
      <link href="2021/03/21/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E9%9B%B6%E6%95%A3%E7%9F%A5%E8%AF%86%E7%82%B9/"/>
      <url>2021/03/21/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E9%9B%B6%E6%95%A3%E7%9F%A5%E8%AF%86%E7%82%B9/</url>
      
        <content type="html"><![CDATA[<ol><li>reshape<pre><code>x_train = x_train.reshape(-1, height, width, channels).astype(&#39;float32&#39;)x_test = x_test.reshape(-1, height, width, channels).astype(&#39;float32&#39;)</code></pre>reshape更改数组形状，astype更改数值类型。</li><li>to_categorical<pre><code>y_train = np_utils.to_categorical(y_train, n_classes) y_test = np_utils.to_categorical(y_test, n_classes)</code></pre>to_categorical就是将类别向量转换为二进制（只有0和1）的矩阵类型表示。其表现为将原有的类别向量转换为独热（onehot）编码的形式。</li><li>keras数据增强<pre><code>datagen = ImageDataGenerator(     rotation_range=10,     width_shift_range=0.1,     height_shift_range=0.1,     horizontal_flip=True,     shear_range=0.1,     zoom_range=0.1,     fill_mode=&#39;nearest&#39;)</code></pre>图像深度学习任务中，面对小数据集，我们往往需要利用Image Data Augmentation图像增广技术来扩充我们的数据集，而keras的内置ImageDataGenerator很好地帮我们实现图像增广。</li><li>keras flow函数<pre><code>for x_batch, y_batch in datagen.flow(x_train, y_train, batch_size=batch_size, shuffle=True):flow(self, X, y, batch_size=32, shuffle=True, seed=None, save_to_dir=None, save_prefix=&#39;&#39;, save_format=&#39;png&#39;)：</code></pre>接收numpy数组和标签为参数,生成经过数据提升或标准化后的batch数据,并在一个无限循环中不断的返回batch数据,flow_from_directory(directory): 以文件夹路径为参数,生成经过数据提升/归一化后的数据,在一个无限循环中无限产生batch数据。</li><li>keras 自定义损失函数<pre><code>def dice_coef(y_true, y_pred, lamb): cross_loss= K.categorical_crossentropy(y_true,y_pred) mmd_loss = keras.losses.kullback_leibler_divergence(y_true,y_pred) return lamb * cross_loss + lamb * mmd_lossdef dice_loss(lamb): def dice(y_true, y_pred):     return dice_coef(y_true, y_pred, lamb) return dicedef my_loss(args, lamb): y_true, y_pred, train_feature, test_feature = args[0], args[1], args[2], args[3] mmd_loss = keras.losses.kullback_leibler_divergence(train_feature,test_feature) cross_loss= K.categorical_crossentropy(y_true,y_pred) return cross_loss + lamb*mmd_loss</code></pre></li><li>keras 训练，使用自定义损失函数<pre><code>model.add(Dense(n_classes,activation=&#39;softmax&#39;))train_features = model.predict(x_train)test_features = model.predict(x_test)train_features = tf.convert_to_tensor(train_features)test_features = tf.convert_to_tensor(test_features)y_pred = model.predict(x_train)y_true = y_trainy_pred = tf.convert_to_tensor(y_pred)y_true = tf.convert_to_tensor(y_true)loss_body = layers.Lambda(my_loss, output_shape=(1,), name=&#39;my_loss&#39;, arguments=&#123;&#39;lamb&#39;:0.5&#125;)([y_true, y_pred, train_features, test_features])model.add(loss_body)sgd = SGD(lr=lr, decay=1e-5, momentum=0.9, nesterov=False)model.compile(loss=&#123;&#39;my_loss&#39;: lambda y_true, y_pred: y_pred&#125;, optimizer=sgd)</code></pre></li><li>保存模型<pre><code>model.save(&#39;路径&#39;)</code></pre></li><li>维度切换<pre><code>x_train = x_train.transpose(0, 3, 1, 2)x_test = x_test.transpose(0, 3, 1, 2)</code></pre>维度切换，将keras的AlextNet的输入的‘数量，长，宽，通道’修改为pytorch的‘数量，通道，长，宽’。</li><li>获取扩增数据，联系第3点<pre><code>x_train_all = []y_train_all = []batches_train=0for x_batch, y_batch in datagen.flow(x_train, y_train, batch_size=batch_size, shuffle=True): x_train_all.append(x_batch) y_train_all.append(y_batch) batches_train+=1 if batches_train &gt;= times * len(x_train) / batch_size:     breakx_train_all = np.concatenate(x_train_all)y_train_all = np.concatenate(y_train_all)x_train_all = torch.tensor(x_train_all)y_train_all = torch.tensor(y_train_all)torch_data_train = GetLoader(x_train, y_train)  #dataloaderdatas_train = DataLoader(torch_data_train, batch_size=batch_size, shuffle=True, drop_last=False, num_workers=0)</code></pre></li><li>pytorch 训练 迁移学习<pre><code>model.train()for e in range(epochs):    print(&#39;epochs:&#39;,e+1)    for j, data_train in enumerate(datas_train, 0):        lr_rate = lr / math.pow((1 +  (e * train_num + j) / epochs*train_num), 0.5)        optimizer = torch.optim.SGD(model.parameters(), lr=lr_rate, momentum=momentum,                                    weight_decay=weight_decay)        torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=&#39;min&#39;, factor=0.1, patience=10, verbose=False,                                                   threshold=0.0001, threshold_mode=&#39;rel&#39;, cooldown=0, min_lr=0,                                                   eps=1e-08)               inputs_train, labels_train = data_train        inputs_train, labels_train = Variable(inputs_train), Variable(labels_train) #variable封装tensor                inputs_test, labels_test = next(data_gen)        inputs_test = torch.tensor(inputs_test)        labels_test = torch.tensor(labels_test)                optimizer.zero_grad()        source_feature,source_result=model(inputs_train)        target_feature,target_result=model(inputs_test)        mmd_loss = mmd.mmd_rbf_noaccelerate(source_feature, target_feature)        loss = F.nll_loss(F.log_softmax(source_result,dim=1), labels_train.long())        lambd = 2 / (1 + math.exp(-10 * (e*train_num+j+1) / (epochs*train_num)))        loss_total = loss + lambd * mmd_loss        loss_total.backward()        optimizer.step()        prediction = torch.argmax(source_result,1)        correct +=(prediction == labels_train).sum().float()        total +=len(labels_train)        accuracy = correct/total        print(&quot;&#123;0:&lt;3d&#125; lr:&#123;1:&lt;.30f&#125;  loss:&#123;2:&lt;.20f&#125;  accuracy:&#123;3:&lt;.20f&#125;&quot;.format(j+1,lr_rate,loss_total,accuracy))</code></pre></li><li>numpy和tensor转化<pre><code>train_feature_batch = torch.tensor(train_feature_batch)train_feature_batch = train_feature_batch.detach().numpy()</code></pre></li><li>cuda选择<pre><code>import osos.environ[&quot;CUDA_VISIBLE_DEVICES&quot;] = &quot;2&quot;</code></pre></li></ol>]]></content>
      
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
            <tag> 迁移学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>基于脑机协同智能的情绪识别</title>
      <link href="2021/03/21/%E5%9F%BA%E4%BA%8E%E8%84%91%E6%9C%BA%E5%8D%8F%E5%90%8C%E6%99%BA%E8%83%BD%E7%9A%84%E6%83%85%E7%BB%AA%E8%AF%86%E5%88%AB/"/>
      <url>2021/03/21/%E5%9F%BA%E4%BA%8E%E8%84%91%E6%9C%BA%E5%8D%8F%E5%90%8C%E6%99%BA%E8%83%BD%E7%9A%84%E6%83%85%E7%BB%AA%E8%AF%86%E5%88%AB/</url>
      
        <content type="html"><![CDATA[<h1 id="基于脑机协同智能的情绪识别"><a href="#基于脑机协同智能的情绪识别" class="headerlink" title="基于脑机协同智能的情绪识别"></a>基于脑机协同智能的情绪识别</h1><blockquote><p>摘 要：面部表情识别是一种直接有效的情感识别模式。机器学习依靠对图像表情进行形式<br>化表征，缺乏大脑的认知表征能力，在小样本数据集或复杂表情（伪装）数据集上识别性能<br>并不理想。针对此问题，将机器人工智能的形式化表征与人脑通用智能的情感认知能力相结<br>合，提出一种脑机协同智能的情绪识别方法。首先，从脑电图信号中提取脑电情感特征以获<br>取大脑对于情绪的认知表征。其次，从情感图像中提取图像的视觉特征以获取机器对于情绪<br>的形式化表征。为增强机器模型的泛化能力，在特征学习中引入样本间的迁移适配。在得到<br>图像视觉特征和脑电情感特征后，采用随机森林回归模型训练得到图像视觉特征与脑电情感<br>特征之间的脑机映射关系。测试图像的图像视觉特征经过脑机映射关系产生虚拟脑电情感特<br>征，然后将虚拟脑电情感特征与图像视觉特征进行融合用于情绪识别。该方法已经在中国情<br>绪图片系统上进行了验证，发现对 7 种情绪的平均识别准确率为 88.51％，相比单纯基于图<br>像的方法提升 3%~5%。</p></blockquote><p>关键词：情绪识别；脑电图信号；脑机协同智能；深度学习</p><p>中图分类号：TP18，TN911.7，R318 </p><p>文献标识码：A </p><p>doi: 10.11959/j.issn.2096−6652.2021000</p>]]></content>
      
      
      
        <tags>
            
            <tag> 情绪识别 </tag>
            
            <tag> 脑机协同智能 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
