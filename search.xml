<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>绘图</title>
      <link href="2021/03/21/%E7%BB%98%E5%9B%BE/"/>
      <url>2021/03/21/%E7%BB%98%E5%9B%BE/</url>
      
        <content type="html"><![CDATA[<ol><li><p>绘图需要引入的包<br>import matplotlib.pyplot as plt<br>import matplotlib<br>from sklearn.metrics import *<br>exp_root = ‘/home/wyh/Python_workspace/DAN_EEGIMAGE_CODE/model_indices/‘<br>import matplotlib.pyplot as plt<br>import numpy as np</p></li><li><p>横向对比图</p><pre><code>def Comparsion_regression(): # 构建数据 x_data = [&#39;愤怒&#39;, &#39;快乐&#39;, &#39;中立&#39;, &#39;悲伤&#39;, &#39;惊奇&#39;, &#39;厌恶&#39;, &#39;恐惧&#39;] y_data1 = [100.00, 92.31, 100.00, 83.33, 83.33, 50.00, 37.50] y_data2 = [100.00, 96.15, 100.00, 83.33, 91.67, 62.50, 37.50] bar_width = 0.4 # Y轴数据使用range(len(x_data), 就是0、1、2... plt.barh(y=range(len(x_data)), width=y_data1, label=&#39;IMAGE&#39;, color=&#39;steelblue&#39;, alpha=0.8, height=bar_width) # Y轴数据使用np.arange(len(x_data))+bar_width, # 就是bar_width、1+bar_width、2+bar_width...这样就和第一个柱状图并列了 plt.barh(y=np.arange(len(x_data)) + bar_width, width=y_data2, label=&#39;EEG-LIKE&#39;, color=&#39;indianred&#39;, alpha=0.8,          height=bar_width) # 在柱状图上显示具体数值, ha参数控制水平对齐方式, va控制垂直对齐方式 for y, x in enumerate(y_data1):     plt.text(x - 6, y - bar_width / 2, &#39;%s&#39; % (x), ha=&#39;center&#39;, va=&#39;bottom&#39;) for y, x in enumerate(y_data2):     plt.text(x - 6, y + bar_width / 2, &#39;%s&#39; % (x), ha=&#39;center&#39;, va=&#39;bottom&#39;) # 为Y轴设置刻度值 plt.yticks(np.arange(len(x_data)) + bar_width / 2, x_data) # 设置标题 plt.title(&quot;Comparison graph of accuracy before and after regression&quot;) # 为两条坐标轴设置名称 plt.xlabel(&quot;Accuracy(%)&quot;) plt.ylabel(&quot;Emotional categories&quot;) plt.subplots_adjust(left=0.2, right=0.8, top=0.8, bottom=0.2) # 显示图例 plt.legend(loc=&#39;upper right&#39;) plt.savefig(&quot;/home/wyh/Python_workspace/DAN_EEGIMAGE_CODE/model_indices/DualChart.jpg&quot;) plt.show()</code></pre><p><img src="/themes/hexo-theme-matery/source/medias/essay/DualChart.jpg" alt="横向对比图"></p></li></ol><p>def plot_confusion_matrix(y_true, y_pred, labels):  # 计算混淆矩阵，以评估分类的准确性。<br>    cmap = plt.cm.binary<br>    cm = confusion_matrix(y_true, y_pred)<br>    tick_marks = np.array(range(len(labels))) + 0.5<br>    np.set_printoptions(precision=2)<br>    cm_normalized = cm.astype(‘float’) / cm.sum(axis=1)[:, np.newaxis]<br>    plt.figure(figsize=(20, 18), dpi=120)<br>    ind_array = np.arange(len(labels))<br>    x, y = np.meshgrid(ind_array, ind_array)<br>    intFlag = 0<br>    for x_val, y_val in zip(x.flatten(), y.flatten()):<br>        #<br>        if (intFlag):<br>            c = cm[y_val][x_val]<br>            plt.text(x_val, y_val, “%d” % (c,), color=’black’, fontsize=20, va=’center’, ha=’center’)</p><pre><code>    else:        c = cm_normalized[y_val][x_val]        if (c &gt; 0.80):            plt.text(x_val, y_val, &quot;%0.2f&quot; % (c * 100,), color=&#39;white&#39;, fontsize=20, va=&#39;center&#39;, ha=&#39;center&#39;)        else:            plt.text(x_val, y_val, &quot;%0.2f&quot; % (c * 100,), color=&#39;black&#39;, fontsize=20, va=&#39;center&#39;, ha=&#39;center&#39;)if (intFlag):    plt.imshow(cm, interpolation=&#39;nearest&#39;, cmap=cmap)else:    plt.imshow(cm_normalized, interpolation=&#39;nearest&#39;, cmap=cmap)plt.gca().set_xticks(tick_marks, minor=True)plt.gca().set_yticks(tick_marks, minor=True)plt.gca().xaxis.set_ticks_position(&#39;none&#39;)plt.gca().yaxis.set_ticks_position(&#39;none&#39;)plt.grid(True, which=&#39;minor&#39;, linestyle=&#39;-&#39;)plt.gcf().subplots_adjust(bottom=0.15)plt.title(&#39;&#39;)plt.colorbar()xlocations = np.array(range(len(labels)))plt.xticks(xlocations, labels, rotation=90, fontsize=20, fontproperties=zhfont1)plt.yticks(xlocations, labels, fontsize=20, fontproperties=zhfont1)plt.ylabel(&#39;真实类别&#39;, fontsize=20, fontproperties=zhfont1)plt.xlabel(&#39;预测类别&#39;, fontsize=20, fontproperties=zhfont1)plt.savefig(exp_root + &#39;knn_confusion_matrix12.jpg&#39;, dpi=600)plt.show()</code></pre><p>def plot_embedding(data, label):<br>    fig = plt.figure(figsize=(5, 5))<br>    type1_x = []<br>    type1_y = []<br>    type2_x = []<br>    type2_y = []<br>    type3_x = []<br>    type3_y = []<br>    type4_x = []<br>    type4_y = []<br>    type5_x = []<br>    type5_y = []<br>    type6_x = []<br>    type6_y = []<br>    type7_x = []<br>    type7_y = []<br>    type8_x = []<br>    type8_y = []</p><pre><code>for i in range(data.shape[0]):    if label[i] == 0:        type1_x.append(data[i][0])        type1_y.append(data[i][1])    if label[i] == 1:        type2_x.append(data[i][0])        type2_y.append(data[i][1])    if label[i] == 2:        type3_x.append(data[i][0])        type3_y.append(data[i][1])    if label[i] == 3:        type4_x.append(data[i][0])        type4_y.append(data[i][1])    if label[i] == 4:        type5_x.append(data[i][0])        type5_y.append(data[i][1])    if label[i] == 5:        type6_x.append(data[i][0])        type6_y.append(data[i][1])    if label[i] == 6:        type7_x.append(data[i][0])        type7_y.append(data[i][1])    if label[i] == 7:        type8_x.append(data[i][0])        type8_y.append(data[i][1])color = plt.cm.Set3(0)color = np.array(color).reshape(1, 4)color1 = plt.cm.Set3(1)color1 = np.array(color1).reshape(1, 4)color2 = plt.cm.Set3(2)color2 = np.array(color2).reshape(1, 4)color3 = plt.cm.Set3(3)color3 = np.array(color3).reshape(1, 4)type1 = plt.scatter(type1_x, type1_y, s=10, c=&#39;r&#39;)type2 = plt.scatter(type2_x, type2_y, s=10, c=&#39;g&#39;)type3 = plt.scatter(type3_x, type3_y, s=10, c=&#39;b&#39;)type4 = plt.scatter(type4_x, type4_y, s=10, c=&#39;k&#39;)type5 = plt.scatter(type5_x, type5_y, s=10, c=&#39;c&#39;)type6 = plt.scatter(type6_x, type6_y, s=10, c=&#39;m&#39;)type7 = plt.scatter(type7_x, type7_y, s=10, c=&#39;y&#39;)type8 = plt.scatter(type8_x, type8_y, s=10, c=color)plt.legend((type1, type2, type3, type4, type5, type6, type7, type8),           (&#39;愤怒&#39;, &#39;厌恶&#39;, &#39;恐惧&#39;, &#39;快乐&#39;, &#39;中立&#39;, &#39;悲伤&#39;, &#39;惊奇&#39;), prop=zhfont1)# loc=(0.77, 0.6))plt.xticks()plt.yticks()return fig</code></pre><p>def visual_features(x_train_img, x_test_img, y_train, y_test):<br>    from sklearn.manifold import TSNE<br>    # 可视化训练/测试数据模式<br>    X_train = x_train_img<br>    Y_train = y_train<br>    tsne = TSNE(n_components=2, init=’pca’, random_state=0)  # 使用TSNE对特征降到二维<br>    reduce_dim_X = tsne.fit_transform(X_train)  # 降维后的数据<br>    # 画图<br>    # plt.subplot(1, 2, 1)<br>    fig = plot_embedding(reduce_dim_X, Y_train)<br>    # 图例过大，保存figure时无法保存完全，故对此参数进行调整<br>    fig.subplots_adjust(right=0.7)<br>    plt.title(‘图像视觉特征可视化’, fontproperties=zhfont1)<br>    plt.savefig(“/home/wyh/Python_workspace/DAN_EEGIMAGE_CODE/model_indices/t-sne_train_image.jpg”)<br>    plt.show()<br>    X_test = x_test_img<br>    Y_test = y_test<br>    tsne = TSNE(n_components=2, init=’pca’, random_state=0)  # 使用TSNE对特征降到二维<br>    reduce_dim_X = tsne.fit_transform(X_test)  # 降维后的数据<br>    # 画图<br>    # plt.subplot(1, 2, 2)<br>    fig = plot_embedding(reduce_dim_X, Y_test)<br>    # 图例过大，保存figure时无法保存完全，故对此参数进行调整<br>    fig.subplots_adjust(right=0.7)<br>    plt.title(‘虚拟脑电情感特征可视化’, fontproperties=zhfont1)<br>    # plt.show()<br>    plt.savefig(“/home/wyh/Python_workspace/DAN_EEGIMAGE_CODE/model_indices/t-sne_train_EEG-LIKE.jpg”)<br>    plt.show()</p><p>def comparison_bar(IMAGE_score, Regression_score, Fusion_score, EEG_score):<br>    X = [‘图像’, ‘回归’, ‘融合’, ‘脑电’]<br>    Y = [0.839080459770114, 0.873563218390804, 0.885057471264367, 0.977011494252873]</p><pre><code>plt.xticks(fontproperties=zhfont1)plt.bar(X, Y, 0.5, color=[&#39;green&#39;, &#39;blue&#39;, &#39;yellow&#39;, &#39;red&#39;])plt.xlabel(&quot;方法&quot;, fontproperties=zhfont1)plt.ylabel(&quot;准确率&quot;, fontproperties=zhfont1)plt.title(&quot;方法对比图&quot;, fontproperties=zhfont1)for a, b in zip(X, Y):    plt.text(a, b, &#39;%.4f&#39; % b, ha=&#39;center&#39;, va=&#39;bottom&#39;, fontsize=11)# plt.show()plt.savefig(&quot;/home/wyh/Python_workspace/DAN_EEGIMAGE_CODE/model_indices/BarChart.jpg&quot;)plt.show()</code></pre><p>def EEG_like(x_test_eeg, y_pred_test):<br>    x_test_eeg_average = x_test_eeg.mean(0)<br>    x_test_like_average = y_pred_test.mean(0)<br>    x1 = range(0, 128)<br>    x2 = range(0, 128)<br>    y1 = x_test_eeg_average<br>    y2 = x_test_like_average<br>    plt.subplot(2, 1, 1)<br>    plt.plot(x1, y1, ‘.-‘)<br>    plt.title(‘脑电情感特征和虚拟脑电情感特征’, fontproperties=zhfont1)<br>    plt.ylabel(‘脑电情感特征’, fontproperties=zhfont1)<br>    # plt.xlabel(‘Time series’,fontproperties=zhfont1)<br>    plt.subplot(2, 1, 2)<br>    plt.plot(x2, y2, ‘.-‘)<br>    plt.xlabel(‘模拟时间序列’, fontproperties=zhfont1)<br>    plt.ylabel(‘虚拟脑电情感特征’, fontproperties=zhfont1)<br>    # plt.show()<br>    plt.savefig(“/home/wyh/Python_workspace/DAN_EEGIMAGE_CODE/model_indices/EEG-LIKE.jpg”)<br>    plt.show()</p>]]></content>
      
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> 绘图 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>深度学习零散知识点</title>
      <link href="2021/03/21/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E9%9B%B6%E6%95%A3%E7%9F%A5%E8%AF%86%E7%82%B9/"/>
      <url>2021/03/21/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E9%9B%B6%E6%95%A3%E7%9F%A5%E8%AF%86%E7%82%B9/</url>
      
        <content type="html"><![CDATA[<p>#深度学习零散知识点</p><ol><li>reshape<pre><code>x_train = x_train.reshape(-1, height, width, channels).astype(&#39;float32&#39;)x_test = x_test.reshape(-1, height, width, channels).astype(&#39;float32&#39;)</code></pre>reshape更改数组形状，astype更改数值类型。</li><li>to_categorical<pre><code>y_train = np_utils.to_categorical(y_train, n_classes) y_test = np_utils.to_categorical(y_test, n_classes)</code></pre>to_categorical就是将类别向量转换为二进制（只有0和1）的矩阵类型表示。其表现为将原有的类别向量转换为独热（onehot）编码的形式。</li><li>keras数据增强<pre><code>datagen = ImageDataGenerator(     rotation_range=10,     width_shift_range=0.1,     height_shift_range=0.1,     horizontal_flip=True,     shear_range=0.1,     zoom_range=0.1,     fill_mode=&#39;nearest&#39;)</code></pre>图像深度学习任务中，面对小数据集，我们往往需要利用Image Data Augmentation图像增广技术来扩充我们的数据集，而keras的内置ImageDataGenerator很好地帮我们实现图像增广。</li><li>keras flow函数<pre><code>for x_batch, y_batch in datagen.flow(x_train, y_train, batch_size=batch_size, shuffle=True):flow(self, X, y, batch_size=32, shuffle=True, seed=None, save_to_dir=None, save_prefix=&#39;&#39;, save_format=&#39;png&#39;)：</code></pre>接收numpy数组和标签为参数,生成经过数据提升或标准化后的batch数据,并在一个无限循环中不断的返回batch数据,flow_from_directory(directory): 以文件夹路径为参数,生成经过数据提升/归一化后的数据,在一个无限循环中无限产生batch数据。</li><li>keras 自定义损失函数<pre><code>def dice_coef(y_true, y_pred, lamb): cross_loss= K.categorical_crossentropy(y_true,y_pred) mmd_loss = keras.losses.kullback_leibler_divergence(y_true,y_pred) return lamb * cross_loss + lamb * mmd_lossdef dice_loss(lamb): def dice(y_true, y_pred):     return dice_coef(y_true, y_pred, lamb) return dicedef my_loss(args, lamb): y_true, y_pred, train_feature, test_feature = args[0], args[1], args[2], args[3] mmd_loss = keras.losses.kullback_leibler_divergence(train_feature,test_feature) cross_loss= K.categorical_crossentropy(y_true,y_pred) return cross_loss + lamb*mmd_loss</code></pre></li><li>keras 训练，使用自定义损失函数<pre><code>model.add(Dense(n_classes,activation=&#39;softmax&#39;))train_features = model.predict(x_train)test_features = model.predict(x_test)train_features = tf.convert_to_tensor(train_features)test_features = tf.convert_to_tensor(test_features)y_pred = model.predict(x_train)y_true = y_trainy_pred = tf.convert_to_tensor(y_pred)y_true = tf.convert_to_tensor(y_true)loss_body = layers.Lambda(my_loss, output_shape=(1,), name=&#39;my_loss&#39;, arguments=&#123;&#39;lamb&#39;:0.5&#125;)([y_true, y_pred, train_features, test_features])model.add(loss_body)sgd = SGD(lr=lr, decay=1e-5, momentum=0.9, nesterov=False)model.compile(loss=&#123;&#39;my_loss&#39;: lambda y_true, y_pred: y_pred&#125;, optimizer=sgd)</code></pre></li><li>保存模型<pre><code>model.save(&#39;路径&#39;)</code></pre></li><li>维度切换<pre><code>x_train = x_train.transpose(0, 3, 1, 2)x_test = x_test.transpose(0, 3, 1, 2)</code></pre>维度切换，将keras的AlextNet的输入的‘数量，长，宽，通道’修改为pytorch的‘数量，通道，长，宽’。</li><li>获取扩增数据，联系第3点<pre><code>x_train_all = []y_train_all = []batches_train=0for x_batch, y_batch in datagen.flow(x_train, y_train, batch_size=batch_size, shuffle=True): x_train_all.append(x_batch) y_train_all.append(y_batch) batches_train+=1 if batches_train &gt;= times * len(x_train) / batch_size:     breakx_train_all = np.concatenate(x_train_all)y_train_all = np.concatenate(y_train_all)x_train_all = torch.tensor(x_train_all)y_train_all = torch.tensor(y_train_all)torch_data_train = GetLoader(x_train, y_train)  #dataloaderdatas_train = DataLoader(torch_data_train, batch_size=batch_size, shuffle=True, drop_last=False, num_workers=0)</code></pre></li><li>pytorch 训练 迁移学习<pre><code>model.train()for e in range(epochs):    print(&#39;epochs:&#39;,e+1)    for j, data_train in enumerate(datas_train, 0):        lr_rate = lr / math.pow((1 +  (e * train_num + j) / epochs*train_num), 0.5)        optimizer = torch.optim.SGD(model.parameters(), lr=lr_rate, momentum=momentum,                                    weight_decay=weight_decay)        torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=&#39;min&#39;, factor=0.1, patience=10, verbose=False,                                                   threshold=0.0001, threshold_mode=&#39;rel&#39;, cooldown=0, min_lr=0,                                                   eps=1e-08)               inputs_train, labels_train = data_train        inputs_train, labels_train = Variable(inputs_train), Variable(labels_train) #variable封装tensor                inputs_test, labels_test = next(data_gen)        inputs_test = torch.tensor(inputs_test)        labels_test = torch.tensor(labels_test)                optimizer.zero_grad()        source_feature,source_result=model(inputs_train)        target_feature,target_result=model(inputs_test)        mmd_loss = mmd.mmd_rbf_noaccelerate(source_feature, target_feature)        loss = F.nll_loss(F.log_softmax(source_result,dim=1), labels_train.long())        lambd = 2 / (1 + math.exp(-10 * (e*train_num+j+1) / (epochs*train_num)))        loss_total = loss + lambd * mmd_loss        loss_total.backward()        optimizer.step()        prediction = torch.argmax(source_result,1)        correct +=(prediction == labels_train).sum().float()        total +=len(labels_train)        accuracy = correct/total        print(&quot;&#123;0:&lt;3d&#125; lr:&#123;1:&lt;.30f&#125;  loss:&#123;2:&lt;.20f&#125;  accuracy:&#123;3:&lt;.20f&#125;&quot;.format(j+1,lr_rate,loss_total,accuracy))</code></pre></li><li>numpy和tensor转化<pre><code>train_feature_batch = torch.tensor(train_feature_batch)train_feature_batch = train_feature_batch.detach().numpy()</code></pre></li><li>cuda选择<pre><code>import osos.environ[&quot;CUDA_VISIBLE_DEVICES&quot;] = &quot;2&quot;</code></pre></li></ol>]]></content>
      
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
            <tag> 迁移学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>基于脑机协同智能的情绪识别</title>
      <link href="2021/03/21/%E5%9F%BA%E4%BA%8E%E8%84%91%E6%9C%BA%E5%8D%8F%E5%90%8C%E6%99%BA%E8%83%BD%E7%9A%84%E6%83%85%E7%BB%AA%E8%AF%86%E5%88%AB/"/>
      <url>2021/03/21/%E5%9F%BA%E4%BA%8E%E8%84%91%E6%9C%BA%E5%8D%8F%E5%90%8C%E6%99%BA%E8%83%BD%E7%9A%84%E6%83%85%E7%BB%AA%E8%AF%86%E5%88%AB/</url>
      
        <content type="html"><![CDATA[<h1 id="基于脑机协同智能的情绪识别"><a href="#基于脑机协同智能的情绪识别" class="headerlink" title="基于脑机协同智能的情绪识别"></a>基于脑机协同智能的情绪识别</h1><blockquote><p>摘 要：面部表情识别是一种直接有效的情感识别模式。机器学习依靠对图像表情进行形式<br>化表征，缺乏大脑的认知表征能力，在小样本数据集或复杂表情（伪装）数据集上识别性能<br>并不理想。针对此问题，将机器人工智能的形式化表征与人脑通用智能的情感认知能力相结<br>合，提出一种脑机协同智能的情绪识别方法。首先，从脑电图信号中提取脑电情感特征以获<br>取大脑对于情绪的认知表征。其次，从情感图像中提取图像的视觉特征以获取机器对于情绪<br>的形式化表征。为增强机器模型的泛化能力，在特征学习中引入样本间的迁移适配。在得到<br>图像视觉特征和脑电情感特征后，采用随机森林回归模型训练得到图像视觉特征与脑电情感<br>特征之间的脑机映射关系。测试图像的图像视觉特征经过脑机映射关系产生虚拟脑电情感特<br>征，然后将虚拟脑电情感特征与图像视觉特征进行融合用于情绪识别。该方法已经在中国情<br>绪图片系统上进行了验证，发现对 7 种情绪的平均识别准确率为 88.51％，相比单纯基于图<br>像的方法提升 3%~5%。</p></blockquote><p>关键词：情绪识别；脑电图信号；脑机协同智能；深度学习</p><p>中图分类号：TP18，TN911.7，R318 </p><p>文献标识码：A </p><p>doi: 10.11959/j.issn.2096−6652.2021000</p>]]></content>
      
      
      
        <tags>
            
            <tag> 情绪识别 </tag>
            
            <tag> 脑机协同智能 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
